syntax = "proto3";
option optimize_for = LITE_RUNTIME;
// all the matrix are stored in row-major order,
// plz see https://en.wikipedia.org/wiki/Row-_and_column-major_order for details

// the definition of "Multi-Head Attention", "Scaled Dot-Product Attention" and
// "Feed-Forward Networks"
// plz see https://arxiv.org/abs/1706.03762 for details

message LlamaDecoderLayer {
  // layer norm before "Llama Attention"
  repeated float attention_norm_scale = 1;
  repeated float attention_project_qkv = 2;

  // "Multi-Head Attention" linearly project weights kernel for output
  // after "Scaled Dot-Product Attention", with shape (hidden_size, hidden_size)
  repeated float attention_output = 5;

  // layer norm before "Llama Feed-Forward Networks"
  repeated float ffn_norm_scale = 6;

  // "Llama MLP layer Networks"
  repeated float gate_up_project_weight = 7;
  repeated float down_project_weight = 9;
}

message LlamaEmbeddingLayer {
  // token embedding table
  // for encoder, it is in [src_vocab_size, hidden_size]
  // so, look it up directly will get the input token embedding
  repeated float token_embedding = 1;
  repeated float position_embedding = 2;
  // the last layer_norm
  repeated float post_norm_scale = 3;
}

message LlamaModelConf {
  int32 hidden_size = 1;
  int32 inner_size = 2;
  int32 max_step = 3;
  int32 head_num = 4;
  int32 layer_num = 5;
  int32 src_padding_id = 6;
  string sampling_method = 7;
  float topp = 8;
  int32 topk = 9;
  int32 eos_id = 10;
  int32 extra_decode_length = 11;
  int32 src_vocab_size = 12;

  int32 beam_size = 13;  // beam size of beam search
  float length_penalty = 14;  // length penalty of beam search
  float diverse_lambda = 15; // diverse beam search lambda
  string act_method = 16; // act method of Llama MLP layer
}

message Llama {
  LlamaEmbeddingLayer src_embedding = 1;
  repeated LlamaDecoderLayer encoder_stack = 2;
  LlamaModelConf model_conf = 3;
}
